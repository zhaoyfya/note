### 收包流程

![[Pasted image 20230312135301.png]]

1.当网络数据帧通过网络传输到达网卡时，网卡会将网络数据帧通过DMA的方式放到环形缓冲区（RingBuffer）中。
> $\color{red}{RingBuffer}$是网卡在启动的时候$\color{red}{分配和初始化}$分配和初始化的$\color{red}{环形缓冲队列}$。当$\color{red}{RingBuffer}$满的时候，新来的数据包就会被$\color{red}{丢弃}$。我们可以通过$\color{red}{ifconfig}$命令查看网卡收发数据包的情况。其中$\color{red}{overruns}$数据项表示当$\color{red}{RingBuffer}$满时，被$\color{red}{丢弃的数据包}$。如果发现出现丢包情况，可以通过$\color{red}{ethtool}$命令来增大$\color{red}{RingBuffer}$长度。

2.当DMA操作完成时，网卡会向CPU发起一个硬中断`，告诉`CPU`有网络数据到达。CPU调用网卡驱动注册的`硬中断响应程序。网卡硬中断响应程序会为网络数据帧创建内核数据结构`sk_buffer`，并将网络数据帧`拷贝`到`sk_buffer`中。然后发起`软中断请求`，通知`内核`有新的网络数据帧到达。
> `sk_buff`缓冲区，是一个维护网络帧结构的`双向链表`，链表中的每一个元素都是一个`网络帧`。虽然 TCP/IP 协议栈分了好几层，但上下不同层之间的传递，实际上只需要操作这个数据结构中的指针，而`无需进行数据复制`。

3.内核线程`ksoftirqd`发现有软中断请求到来，随后调用网卡驱动注册的`poll函数`，`poll函数`将`sk_buffer`中的网络数据包送到内核协议栈中注册的`ip_rcv函数`中。
>`每个CPU`会绑定`一个ksoftirqd`内核线程`专门`用来处理`软中断响应`。2个 CPU 时，就会有 `ksoftirqd/0` 和 `ksoftirqd/1`这两个内核线程。
  **这里有个事情需要注意下：** 网卡接收到数据后，当`DMA拷贝完成`时，向CPU发出`硬中断`，这时`哪个CPU`上响应了这个`硬中断`，那么在网卡`硬中断响应程序`中发出的`软中断请求`也会在`这个CPU绑定的ksoftirqd线程`中响应。所以如果发现Linux软中断，CPU消耗都`集中在一个核上`的话，那么就需要调整硬中断的`CPU亲和性`，来将硬中断`打散`到`不通的CPU核`上去。

4.在`ip_rcv函数`中也就是上图中的`网络层`，`取出`数据包的`IP头`，判断该数据包下一跳的走向，如果数据包是发送给本机的，则取出传输层的协议类型（`TCP`或者`UDP`)，并`去掉`数据包的`IP头`，将数据包交给上图中得`传输层`处理。
> 传输层的处理函数：`TCP协议`对应内核协议栈中注册的`tcp_rcv函数`，`UDP协议`对应内核协议栈中注册的`udp_rcv函数`。

5.当我们采用的是`TCP协议`时，数据包到达传输层时，会在内核协议栈中的`tcp_rcv函数`处理，在tcp_rcv函数中`去掉`TCP头，根据`四元组（源IP，源端口，目的IP，目的端口）`查找`对应的Socket`，如果找到对应的Socket则将网络数据包中的传输数据拷贝到`Socket`中的`接收缓冲区`中。如果没有找到，则发送一个`目标不可达`的`icmp`包。

6.内核在接收网络数据包时所做的工作我们就介绍完了，现在我们把视角放到应用层，当我们程序通过系统调用`read`读取`Socket接收缓冲区`中的数据时，如果接收缓冲区中`没有数据`，那么应用程序就会在系统调用上`阻塞`，直到Socket接收缓冲区`有数据`，然后`CPU`将`内核空间`（Socket接收缓冲区）的数据`拷贝`到`用户空间`，最后系统调用`read返回`，应用程序`读取`数据。

流程总结：
网卡收到数据->DMA将网卡数据放到环形缓冲区（RingBuffer）->硬中断触发通知CPU有数据到了，并将数据拷贝到内核结构sk_buffer缓冲区->软中断触发创建ksoftirqd线程处理数据包（ip_rcv->tcp_rcv/udp_rcv）->根据四元组查找socket缓冲区，并将应用层数据拷贝到socket缓冲区等到应用程序read。

### 发送流程

![[Pasted image 20230312143651.png]]

1.当我们在应用程序中调用`send`系统调用发送数据时，由于是系统调用所以线程会发生一次用户态到内核态的转换，在内核中首先根据`fd`将真正的Socket找出，这个Socket对象中记录着各种协议栈的函数地址，然后构造`struct msghdr`对象，将用户需要发送的数据全部封装在这个`struct msghdr`结构体中。

2.调用内核协议栈函数`inet_sendmsg`，发送流程进入内核协议栈处理。在进入到内核协议栈之后，内核会找到Socket上的具体协议的发送函数。
> 比如：我们使用的是`TCP协议`，对应的`TCP协议`发送函数是`tcp_sendmsg`，如果是`UDP协议`的话，对应的发送函数为`udp_sendmsg`。

3.在`TCP协议`的发送函数`tcp_sendmsg`中，创建内核数据结构`sk_buffer`,将`struct msghdr`结构体中的发送数据`拷贝`到`sk_buffer`中。调用`tcp_write_queue_tail`函数获取`Socket`发送队列中的队尾元素，将新创建的`sk_buffer`添加到`Socket`发送队列的尾部。
> `Socket`的发送队列是由`sk_buffer`组成的一个`双向链表`。
> 发送流程走到这里，用户要发送的数据总算是从`用户空间`拷贝到了`内核`中，这时虽然发送数据已经`拷贝`到了内核`Socket`中的`发送队列`中，但并不代表内核会开始发送，因为`TCP协议`的`流量控制`和`拥塞控制`，用户要发送的数据包`并不一定`会立马被发送出去，需要符合`TCP协议`的发送条件。如果`没有达到发送条件`，那么本次`send`系统调用就会直接返回。

4.如果符合发送条件，则开始调用`tcp_write_xmit`内核函数。在这个函数中，会循环获取`Socket`发送队列中待发送的`sk_buffer`，然后进行`拥塞控制`以及`滑动窗口的管理`。  

5.将从`Socket`发送队列中获取到的`sk_buffer`重新`拷贝一份`，设置`sk_buffer副本`中的`TCP HEADER`。
> `sk_buffer` 内部其实包含了网络协议中所有的 `header`。在设置 `TCP HEADER`的时候，只是把指针指向 `sk_buffer`的合适位置。后面再设置 `IP HEADER`的时候，在把指针移动一下就行，避免频繁的内存申请和拷贝，效率很高。
![[Pasted image 20230312153824.png]]
> **为什么不直接使用`Socket`发送队列中的`sk_buffer`而是需要拷贝一份呢？**因为`TCP协议`是支持`丢包重传`的，在没有收到对端的`ACK`之前，这个`sk_buffer`是不能删除的。内核每次调用网卡发送数据的时候，实际上传递的是`sk_buffer`的`拷贝副本`，当网卡把数据发送出去后，`sk_buffer`拷贝副本会被释放。当收到对端的`ACK`之后，`Socket`发送队列中的`sk_buffer`才会被真正删除。

6.当设置完`TCP头`后，内核协议栈`传输层`的事情就做完了，下面通过调用`ip_queue_xmit`内核函数，正式来到内核协议栈`网络层`的处理。
> 通过`route`命令可以查看本机路由配置。
> 如果你使用 `iptables`配置了一些规则，那么这里将检测`是否命中`规则。如果你设置了非常`复杂的 netfilter 规则`，在这个函数里将会导致你的线程 `CPU 开销`会`极大增加`。
7.将`sk_buffer`中的指针移动到`IP头`位置上，设置`IP头`。

8.执行`netfilters`过滤。过滤通过之后，如果数据大于 `MTU`的话，则执行分片。
  
9.检查`Socket`中是否有缓存路由表，如果没有的话，则查找路由项，并缓存到`Socket`中。接着在把路由表设置到`sk_buffer`中。  

10.内核协议栈`网络层`的事情处理完后，现在发送流程进入了到了`邻居子系统`，`邻居子系统`位于内核协议栈中的`网络层`和`网络接口层`之间，用于发送`ARP请求`获取`MAC地址`，然后将`sk_buffer`中的指针移动到`MAC头`位置，填充`MAC头`。

11.经过`邻居子系统`的处理，现在`sk_buffer`中已经封装了一个完整的`数据帧`，随后内核将`sk_buffer`交给`网络设备子系统`进行处理。`网络设备子系统`主要做以下几项事情：
- 选择发送队列（`RingBuffer`）。因为网卡拥有多个发送队列，所以在发送前需要选择一个发送队列。
- 将`sk_buffer`添加到发送队列中。
- 循环从发送队列（`RingBuffer`）中取出`sk_buffer`，调用内核函数`sch_direct_xmit`发送数据，其中会调用`网卡驱动程序`来发送数据。
> 以上过程全部是用户线程的内核态在执行，占用的CPU时间是系统态时间(`sy`)，当分配给用户线程的`CPU quota`用完的时候，会触发`NET_TX_SOFTIRQ`类型的软中断，内核线程`ksoftirqd`会响应这个软中断，并执行`NET_TX_SOFTIRQ`类型的软中断注册的回调函数`net_tx_action`，在回调函数中会执行到驱动程序函数 `dev_hard_start_xmit`来发送数据。
> **注意：当触发`NET_TX_SOFTIRQ`软中断来发送数据时，后边消耗的 CPU 就都显示在 `si`这里了，不会消耗用户进程的系统态时间（`sy`）了。**
> 从这里可以看到网络包的发送过程和接受过程是不同的，在介绍网络包的接受过程时，我们提到是通过触发`NET_RX_SOFTIRQ`类型的软中断在内核线程`ksoftirqd`中执行`内核网络协议栈`接受数据。而在网络数据包的发送过程中是`用户线程的内核态`在执行`内核网络协议栈`，只有当线程的`CPU quota`用尽时，才触发`NET_TX_SOFTIRQ`软中断来发送数据。
> 在整个网络包的发送和接受过程中，`NET_TX_SOFTIRQ`类型的软中断只会在发送网络包时并且当用户线程的`CPU quota`用尽时，才会触发。剩下的接受过程中触发的软中断类型以及发送完数据触发的软中断类型均为`NET_RX_SOFTIRQ`。所以这就是你在服务器上查看 `/proc/softirqs`，一般 `NET_RX`都要比 `NET_TX`大很多的的原因。

12.现在发送流程终于到了网卡真实发送数据的阶段，前边我们讲到无论是用户线程的内核态还是触发`NET_TX_SOFTIRQ`类型的软中断在发送数据的时候最终会调用到网卡的驱动程序函数`dev_hard_start_xmit`来发送数据。在网卡驱动程序函数`dev_hard_start_xmit`中会将`sk_buffer`映射到网卡可访问的`内存 DMA 区域`，最终网卡驱动程序通过`DMA`的方式将`数据帧`通过物理网卡发送出去。

16.当数据发送完毕后，还有最后一项重要的工作，就是清理工作。数据发送完毕后，网卡设备会向`CPU`发送一个硬中断，`CPU`调用网卡驱动程序注册的`硬中断响应程序`，在硬中断响应中触发`NET_RX_SOFTIRQ`类型的软中断，在软中断的回调函数`igb_poll`中清理释放 `sk_buffer`，清理`网卡`发送队列（`RingBuffer`），解除 DMA 映射。
> 无论`硬中断`是因为`有数据要接收`，还是说`发送完成通知`，从硬中断触发的软中断都是 `NET_RX_SOFTIRQ`。
> 这里释放清理的只是`sk_buffer`的副本，真正的`sk_buffer`现在还是存放在`Socket`的发送队列中。前面在`传输层`处理的时候我们提到过，因为传输层需要`保证可靠性`，所以 `sk_buffer`其实还没有删除。它得等收到对方的 ACK 之后才会真正删除。

流程总结：
调用send陷入内核态，通过fd找到对应的socket(socket记录着相应的协议栈函数)，构造`struct msghdr`对象，将用户数据封装到`struct msghdr`结构体中->创建sk_buffer对象，将`struct msghdr`数据拷贝到sk_buffer中，然后将sk_buffer添加到发送队列（尾部添加）->不满足TCP的流量控制和拥塞控制，本次send直接返回。

满足发送条件，循环获取socket发送队列的sk_buffer进行拥塞控制和滑动窗口管理->拷贝socket发送队列中的sk_buffer，在副本中封装tcp_header->封装ip_header，netfilter过滤，缓存路有表到sokcet和sk_buffer中->邻居子系统（mac地址获取，封装eatcher_header），至此一个完整的数据帧已经封装完成了->网络设备子系统，选择发送队列（Ringbuffer），将sk_buffer添加到发送队列，循环发送数据。

**注**：满足发送条件的时候拷贝sk_buffer是因为要考虑丢包重传的因素，只有当收到当前包的ACK响应后sokcet中真正的sk_buffer才会被清理，而每次发送过后清理的都是sk_buffer的副本